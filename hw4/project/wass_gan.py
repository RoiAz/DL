import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Callable
from torch.utils.data import DataLoader
from torch.optim.optimizer import Optimizer
import project.vanilla_gan as gan_vanila

class Discriminator(gan_vanila.Discriminator):
    def __init__(self, in_size):
        super(Discriminator,self).__init__(in_size)


class Generator(gan_vanila.Generator):
    def __init__(self, z_dim, featuremap_size=4, out_channels=3):
        super(Generator,self).__init__(z_dim, featuremap_size, out_channels)

def discriminator_loss_fn(y_data, y_generated, data_label=0, label_noise=0.0):
    """
    just return the mean(gred)-mean(data)
    """
    assert data_label == 1 or data_label == 0
    """
    # TODO:
    #  Implement the discriminator loss.
    #  See pytorch's BCEWithLogitsLoss for a numerically stable implementation.
    # ====== YOUR CODE: ======
    # set loss function and random images
    device =y_data.device
    loss_func = nn.BCEWithLogitsLoss()
    data_noise = (torch.rand(y_data.shape) * label_noise) - (label_noise/2)
    gene_noise = (label_noise * torch.rand(y_generated.shape)) - (label_noise/2)
    
    # get data loss
    target = (data_label + data_noise).to(device)
    loss_data = loss_func(input=y_data,target=target).to(device)
    
    
    # get gene loss
    target = (gene_noise + 1 - data_label).to(device)
    loss_generated = loss_func(input=y_generated,target=target).to(device)
    # ========================
    """
    return torch.mean(y_generated)-torch.mean(y_data)

def generator_loss_fn(y_generated, data_label=0):
    """
    Computes the loss of the generator given generated data using a
    binary cross-entropy metric.
    This is the loss used to update the Generator parameters.
    :param y_generated: Discriminator class-scores of instances of data
    generated by the generator, shape (N,).
    :param data_label: 0 or 1, label of instances coming from the real dataset.
    :return: The generator loss.
    """
    assert data_label == 1 or data_label == 0
    # TODO:
    #  Implement the Generator loss.
    #  Think about what you need to compare the input to, in order to
    #  formulate the loss in terms of Binary Cross Entropy.
    # ====== YOUR CODE: ======
    """
    device = y_generated.device
    loss_func = nn.BCEWithLogitsLoss()
    
    loss = loss_func(y_generated, torch.full_like(y_generated,data_label, device=device))
    """
    loss = -torch.mean(y_generated)
    # ========================
    return loss

def train_batch(
    dsc_model: Discriminator,
    gen_model: Generator,
    dsc_loss_fn: Callable,
    gen_loss_fn: Callable,
    dsc_optimizer: Optimizer,
    gen_optimizer: Optimizer,
    x_data: DataLoader,
):

    """
    Trains a GAN for over one batch, updating both the discriminator and
    generator.
    :return: The discriminator and generator losses.
    """

    # TODO: Discriminator update
    #  1. Show the discriminator real and generated data
    #  2. Calculate discriminator loss
    #  3. Update discriminator parameters
    # ====== YOUR CODE: ======
    def wass_gan_hyperparams():
        hypers = dict(
            batch_size=0,
            z_dim=0,
            data_label=0,
            label_noise=0.0,
            discriminator_optimizer=dict(
                type="",  # Any name in nn.optim like SGD, Adam
                lr=0.0,
                # You an add extra args for the optimizer here
            ),
            generator_optimizer=dict(
                type="",  # Any name in nn.optim like SGD, Adam
                lr=0.0,
                # You an add extra args for the optimizer here
            ),
        )
        type = 'RMSprop'
        hypers = dict(
            batch_size=32,
            z_dim=128,
            data_label=1,
            label_noise=0.0002,
            discriminator_optimizer=dict(
                type=type,
                lr=0.000035 ,
            ),
            generator_optimizer=dict(
                type=type,
                lr=0.0001 ,
            ),
            N = 5 # for each generator update, we will to 5 discriminator updates.
        )
        return hypers

    N = wass_gan_hyperparams()['N']
    # for each generator update, we will to N discriminator updates.
    
    for i in range(N):
        dsc_optimizer.zero_grad()

        # gene forward date
        gene_data = gen_model.sample(x_data.shape[0], with_grad=True)
        gene_pre = dsc_model(gene_data.detach())

        # forward
        data_pre = dsc_model(x_data)

        #get loss
        dsc_loss = dsc_loss_fn(data_pre, gene_pre)
        
        # optimize
        dsc_loss.backward()
        dsc_optimizer.step()

        for p in dsc_model.parameters():
            p.data.clamp_(-0.01, 0.01)
    # ========================

    # TODO: Generator update
    #  1. Show the discriminator generated data
    #  2. Calculate generator loss
    #  3. Update generator parameters
    # ====== YOUR CODE: ======
    gen_optimizer.zero_grad()
    sampels = gen_model.sample(x_data.shape[0], with_grad=True)
    gen_label = dsc_model(sampels)
    gen_loss = gen_loss_fn(gen_label)
    gen_loss.backward()
    gen_optimizer.step()
    # ========================

    return dsc_loss.item(), gen_loss.item()

def save_checkpoint(gen_model, dsc_losses, gen_losses, checkpoint_file):
    print("w_save")
    return gan_vanila.save_checkpoint(gen_model, dsc_losses, gen_losses, checkpoint_file)
