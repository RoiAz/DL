{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 3: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll implement a small comparative-analysis project, heavily based on the materials from the tutorials and homework.\n",
    "\n",
    "You must **choose one** of the project options specified below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should implement the code which displays your results in this notebook, and add any additional code files for your implementation in the `project/` directory. You can import these files here, as we do for the homeworks.\n",
    "- Running this notebook should not perform any training - load your results from some output files and display them here. The notebook must be runnable from start to end without errors.\n",
    "- You must include a detailed write-up (in the notebook) of what you implemented and how. \n",
    "- Explain the structure of your code and how to run it to reproduce your results.\n",
    "- Explicitly state any external code you used, including built-in pytorch models and code from the course tutorials/homework.\n",
    "- Analyze your numerical results, explaining **why** you got these results (not just specifying the results).\n",
    "- Where relevant, place all results in a table or display them using a graph.\n",
    "- Before submitting, make sure all files which are required to run this notebook are included in the generated submission zip.\n",
    "- Try to keep the submission file size under 10MB. Do not include model checkpoint files, dataset files, or any other non-essentials files. Instead include your results as images/text files/pickles/etc, and load them for display in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Self-Attention and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Tutorials 6 and 7, we'll implement and train an improved sentiment analysis model.\n",
    "We'll use self-attention instead of RNNs and incorporate pre-trained word embeddings.\n",
    "\n",
    "In tutorial 6 we saw that we can train word embeddings together with the model.\n",
    "Although this produces embeddings which are customized to the specific task at hand,\n",
    "it also greatly increases training time.\n",
    "A common technique is to use pre-trained word embeddings.\n",
    "This is essentially a large mapping from words (e.g. in english) to some\n",
    "high-dimensional vector, such that semantically similar words have an embedding that is\n",
    "\"close\" by some metric (e.g. cosine distance).\n",
    "Use the [GloVe](https://nlp.stanford.edu/projects/glove/) 6B embeddings for this purpose.\n",
    "You can load these vectors into the weights of an `nn.Embedding` layer.\n",
    "\n",
    "In tutorial 7 we learned how attention can be used to learn to predict a relative importance\n",
    "for each element in a sequence, compared to the other elements.\n",
    "Here, we'll replace the RNN with self-attention only approach similar to Transformer models, roughly based on [this paper](https://www.aclweb.org/anthology/W18-6219.pdf).\n",
    "After embedding each word in the sentence using the pre-trained word-embedding a positional-encoding vector is added to provide each word in the sentence a unique value based on it's location.\n",
    "One or more self-attention layers are then applied to the results, to obtain an importance weighting for each word.\n",
    "Then we classify the sentence based on the average these weighted encodings.\n",
    "\n",
    "\n",
    "Now, using these approaches, you need to:\n",
    "\n",
    "- Implement a **baseline** model: Use pre-trained embeddings with an RNN-based model.\n",
    "You can use LSTM/GRU or bi-directional versions of these, in a way very similar to what we implemented in the tutorial.\n",
    "-  Implement an **improved** model: Based on the self-attention approach, implement an attention-based sentiment analysis model that has 1-2 self-attention layers instead of an RNN. You should use the same pre-trained word embeddings for this model.\n",
    "- You can use pytorch's built-in RNNs, attention layers, etc.\n",
    "- For positional encoding you can use the sinosoidal approach described in the paper (first proposed [here](https://arxiv.org/pdf/1706.03762.pdf)). You can use existing online implementations (even though it's straightforward to implement). \n",
    "- You can use the SST database as shown in the tutorial.\n",
    "\n",
    "Your results should include:\n",
    "- Everything written in the **Guidelines** above.\n",
    "- A comparative analysis: compare the baseline to the improved model. Compare in terms of overall classification accuracy and show a multiclass confusion matrix.\n",
    "- Visualize of the attention maps for a few movie reviews from each class, and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrally-Normalized Wasserstein GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW3 we implemented a simple GANs from scratch, using an approach very similar to the original GAN paper. However, the results left much to be desired and we discovered first-hand how hard it is to train GANs due to their inherent instability.\n",
    "\n",
    "One of the prevailing approaches for improving training stability for GANs is to use a technique called [Spectral Normalization](https://arxiv.org/pdf/1802.05957.pdf) to normalize the largest singular value of a weight matrix so that it equals 1.\n",
    "This approach is generally applied to the discriminator's weights in order to stabilize training. The resulting model is sometimes referred to as a SN-GAN.\n",
    "See Appendix A in the linked paper for the exact algorithm. You can also use pytorch's `spectral_norm`.\n",
    "\n",
    "Another very common improvement to the vanilla GAN is known a [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) (WGAN). It uses a simple modification to the loss function, with strong theoretical justifications based on the Wasserstein (earth-mover's) distance.\n",
    "See also [here](https://developers.google.com/machine-learning/gan/loss) for a brief explanation of this loss function.\n",
    "\n",
    "One problem with generative models for images is that it's difficult to objectively assess the quality of the resulting images.\n",
    "To also obtain a quantitative score for the images generated by each model,\n",
    "we'll use the [Inception Score](https://arxiv.org/pdf/1606.03498.pdf).\n",
    "This uses a pre-trained Inception CNN model on the generated images and computes a score based on the predicted probability for each class.\n",
    "Although not a perfect proxy for subjective quality, it's commonly used a way to compare generative models.\n",
    "You can use an implementation of this score that you find online, e.g. [this one](https://github.com/sbarratt/inception-score-pytorch) or implement it yourself.\n",
    "\n",
    "Based on the linked papers, add Spectral Normalization and the Wassertein loss to your GAN from HW3.\n",
    "Compare between:\n",
    "- The baseline model (vanilla GAN)\n",
    "- SN-GAN (vanilla + Spectral Normalization)\n",
    "- WGAN (using Wasserstein Loss)\n",
    "- Optional: SN+WGAN, i.e. a combined model using both modifications.\n",
    "\n",
    "As a dataset, you can use [LFW](http://vis-www.cs.umass.edu/lfw/) as in HW3 or [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), or even choose a custom dataset (note that there's a dataloder for CelebA in `torchvision`). \n",
    "\n",
    "Your results should include:\n",
    "- Everything written in the **Guidelines** above.\n",
    "- A comparative analysis between the baseline and the other models. Compare:\n",
    "  - Subjective quality (show multiple generated images from each model)\n",
    "  - Inception score (can use a subset of the data).\n",
    "- You should show substantially improved subjective visual results with these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This is where you should write your explanations and implement the code to display the results.\n",
    "See guidelines about what to include in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labbrys\n",
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cs236781.plot as plot\n",
    "import cs236781.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create file for the checkpoint and images, we will save the resoult there\"\"\"\n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "if not os.path.exists('project_imgs'):\n",
    "    os.makedirs('project_imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/topaz.aharon/.pytorch-datasets/lfw-bush.zip exists, skipping download.\n",
      "Extracting /home/topaz.aharon/.pytorch-datasets/lfw-bush.zip...\n",
      "Extracted 531 to /home/topaz.aharon/.pytorch-datasets/lfw/George_W_Bush\n",
      "we working on device:  cuda\n",
      "\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load the data \"\"\"\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "#load device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CUSTOM_DATA_URL = None\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "if CUSTOM_DATA_URL is None:\n",
    "    DATA_URL = 'http://vis-www.cs.umass.edu/lfw/lfw-bush.zip'\n",
    "else:\n",
    "    DATA_URL = CUSTOM_DATA_URL\n",
    "\n",
    "# download data\n",
    "_, dataset_dir = cs236781.download.download_data(out_path=DATA_DIR, url=DATA_URL, extract=True, force=False)\n",
    "\n",
    "\n",
    "im_size = 64\n",
    "tf = T.Compose([\n",
    "    # Resize to constant spatial dimensions\n",
    "    T.Resize((im_size, im_size)),\n",
    "    # PIL.Image -> torch.Tensor\n",
    "    T.ToTensor(),\n",
    "    # Dynamic range [0,1] -> [-1, 1]\n",
    "    T.Normalize(mean=(.5,.5,.5), std=(.5,.5,.5)),\n",
    "])\n",
    "\n",
    "ds_gwb = ImageFolder(os.path.dirname(dataset_dir), tf)\n",
    "print(\"we working on device: \", device)\n",
    "print(\"\\n##################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from project.train_gan_model import train_gan_model\n",
    "# from  project.answers import *\n",
    "\n",
    "from project.train_gan_model import *\n",
    "from project.score_inception import inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n",
      "{'batch_size': 32, 'z_dim': 128, 'data_label': 1, 'label_noise': 0.2, 'discriminator_optimizer': {'type': 'SGD', 'lr': 0.0075}, 'generator_optimizer': {'type': 'Adam', 'lr': 0.001, 'betas': (0.5, 0.999)}}\n",
      "<module 'project.vanilla_gan' from '/home/topaz.aharon/DL/hw4/project/vanilla_gan.py'>\n",
      "train_model_222\n",
      "Generator(\n",
      "  (reconstructor): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.05)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.05)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.05)\n",
      "    (9): ConvTranspose2d(128, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "    (10): Tanh()\n",
      "  )\n",
      "  (feats): Linear(in_features=128, out_features=16384, bias=True)\n",
      ")\n",
      "*** Images Generated from model of the v_gan:\n",
      "The size of samples:tensor([[[[-8.6214e-01, -9.6418e-01, -8.9193e-01,  ..., -9.5149e-01,\n",
      "           -5.5896e-01, -7.5832e-01],\n",
      "          [-9.2353e-01, -9.4616e-01, -9.7147e-01,  ..., -9.6199e-01,\n",
      "           -8.9403e-01, -7.3908e-01],\n",
      "          [-9.1657e-01, -9.7305e-01, -9.4818e-01,  ..., -9.6550e-01,\n",
      "           -8.2170e-01, -8.0471e-01],\n",
      "          ...,\n",
      "          [-8.5379e-01, -9.0819e-01, -9.7485e-01,  ..., -8.4563e-01,\n",
      "           -7.5277e-01, -6.1099e-01],\n",
      "          [-7.8183e-01, -9.6061e-01, -8.6590e-01,  ..., -9.1864e-01,\n",
      "           -1.9902e-01, -6.2280e-01],\n",
      "          [-7.6859e-01, -5.6942e-01, -9.2448e-01,  ..., -4.2663e-01,\n",
      "           -6.0154e-01, -3.3188e-01]],\n",
      "\n",
      "         [[-8.7547e-01, -9.7064e-01, -9.5339e-01,  ..., -9.5238e-01,\n",
      "           -8.4513e-01, -5.2892e-01],\n",
      "          [-9.6696e-01, -9.3754e-01, -9.8751e-01,  ..., -8.8551e-01,\n",
      "           -8.8864e-01, -7.3489e-01],\n",
      "          [-9.0019e-01, -9.2027e-01, -9.8831e-01,  ..., -9.1320e-01,\n",
      "           -8.8261e-01, -6.2351e-01],\n",
      "          ...,\n",
      "          [-9.6826e-01, -9.3237e-01, -9.9080e-01,  ..., -8.5313e-01,\n",
      "           -8.1344e-01, -6.6368e-01],\n",
      "          [-8.4260e-01, -8.4159e-01, -8.9139e-01,  ..., -8.7418e-01,\n",
      "           -8.0574e-01, -3.4388e-01],\n",
      "          [-5.8783e-01, -7.0821e-01, -8.3512e-01,  ..., -5.8313e-01,\n",
      "           -5.6306e-01, -2.8513e-01]],\n",
      "\n",
      "         [[-9.2557e-01, -8.9648e-01, -9.7683e-01,  ..., -8.8353e-01,\n",
      "           -8.8584e-01, -5.5020e-01],\n",
      "          [-9.3159e-01, -9.8510e-01, -9.9519e-01,  ..., -9.7139e-01,\n",
      "           -9.4903e-01, -8.4429e-01],\n",
      "          [-9.8318e-01, -9.7753e-01, -9.8951e-01,  ..., -9.6320e-01,\n",
      "           -9.1368e-01, -7.8829e-01],\n",
      "          ...,\n",
      "          [-9.0135e-01, -9.7530e-01, -9.9495e-01,  ..., -9.6441e-01,\n",
      "           -9.5225e-01, -7.8843e-01],\n",
      "          [-9.7452e-01, -9.1990e-01, -9.8467e-01,  ..., -8.3226e-01,\n",
      "           -8.4070e-01, -6.0841e-01],\n",
      "          [-6.9308e-01, -8.6809e-01, -9.5043e-01,  ..., -8.0229e-01,\n",
      "           -6.9951e-01, -4.6431e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.4991e-01, -9.1908e-01, -7.8326e-01,  ..., -5.4582e-01,\n",
      "           -1.1726e-01, -5.7453e-01],\n",
      "          [-8.6223e-01, -8.8661e-01, -9.5197e-01,  ..., -8.0583e-04,\n",
      "            8.7015e-02, -2.0522e-01],\n",
      "          [-7.9424e-01, -9.2745e-01, -8.5824e-01,  ...,  6.3948e-01,\n",
      "            2.9712e-01, -4.6898e-01],\n",
      "          ...,\n",
      "          [-7.9280e-01, -8.8520e-01, -9.6519e-01,  ..., -7.9963e-01,\n",
      "           -7.3894e-01, -5.9755e-01],\n",
      "          [-7.8259e-01, -9.5269e-01, -8.1280e-01,  ..., -8.9056e-01,\n",
      "           -2.0574e-01, -6.1580e-01],\n",
      "          [-7.6668e-01, -5.6549e-01, -9.2122e-01,  ..., -3.6506e-01,\n",
      "           -5.7203e-01, -3.2416e-01]],\n",
      "\n",
      "         [[-7.7391e-01, -9.4637e-01, -9.1004e-01,  ...,  2.6792e-02,\n",
      "           -6.5389e-01, -4.3964e-01],\n",
      "          [-9.2214e-01, -8.9196e-01, -9.6957e-01,  ...,  4.3277e-01,\n",
      "           -2.8847e-01, -6.8379e-01],\n",
      "          [-7.9778e-01, -8.0008e-01, -9.6154e-01,  ...,  4.0706e-01,\n",
      "            4.5378e-01, -2.3927e-01],\n",
      "          ...,\n",
      "          [-9.5941e-01, -8.9267e-01, -9.8258e-01,  ..., -7.7239e-01,\n",
      "           -7.9899e-01, -6.1872e-01],\n",
      "          [-8.5170e-01, -8.0179e-01, -8.2513e-01,  ..., -7.4643e-01,\n",
      "           -6.6049e-01, -3.3690e-01],\n",
      "          [-5.8612e-01, -7.0618e-01, -8.3049e-01,  ..., -5.6243e-01,\n",
      "           -5.3780e-01, -2.7646e-01]],\n",
      "\n",
      "         [[-8.8192e-01, -8.6242e-01, -9.7182e-01,  ...,  3.1340e-01,\n",
      "            3.8638e-01, -2.3271e-01],\n",
      "          [-8.7982e-01, -9.7145e-01, -9.8841e-01,  ...,  8.8202e-02,\n",
      "           -5.5837e-01, -5.6848e-01],\n",
      "          [-9.5756e-01, -9.4475e-01, -9.6027e-01,  ...,  2.7054e-01,\n",
      "            6.5112e-01, -4.4432e-01],\n",
      "          ...,\n",
      "          [-8.5206e-01, -9.6255e-01, -9.8702e-01,  ..., -9.3258e-01,\n",
      "           -9.2507e-01, -7.5836e-01],\n",
      "          [-9.6824e-01, -8.7939e-01, -9.7761e-01,  ..., -7.5924e-01,\n",
      "           -7.6585e-01, -5.7876e-01],\n",
      "          [-6.9084e-01, -8.6654e-01, -9.4710e-01,  ..., -7.6658e-01,\n",
      "           -6.7447e-01, -4.5553e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.0771e-01,  8.7376e-01,  9.9850e-01,  ..., -9.7913e-01,\n",
      "           -6.5528e-01, -8.1210e-01],\n",
      "          [ 9.2093e-01,  9.3662e-01,  9.5087e-01,  ..., -9.8172e-01,\n",
      "           -9.3647e-01, -7.9293e-01],\n",
      "          [ 9.9534e-01,  9.5356e-01,  9.9908e-01,  ..., -9.8507e-01,\n",
      "           -8.8431e-01, -8.7277e-01],\n",
      "          ...,\n",
      "          [-4.8449e-01, -6.2282e-01, -7.4517e-01,  ..., -7.6524e-01,\n",
      "           -7.5878e-01, -5.6600e-01],\n",
      "          [-5.0468e-01, -7.8882e-01, -3.3028e-01,  ..., -8.8388e-01,\n",
      "           -1.4338e-01, -5.7111e-01],\n",
      "          [-5.5767e-01, -2.9902e-01, -7.5976e-01,  ..., -3.7045e-01,\n",
      "           -5.9145e-01, -3.2118e-01]],\n",
      "\n",
      "         [[ 6.5943e-01,  9.1934e-01,  9.9876e-01,  ..., -9.7882e-01,\n",
      "           -8.9216e-01, -6.1470e-01],\n",
      "          [ 8.6254e-01,  9.6978e-01,  9.6152e-01,  ..., -9.2898e-01,\n",
      "           -9.3171e-01, -7.7841e-01],\n",
      "          [ 9.7662e-01,  9.9747e-01,  9.9996e-01,  ..., -9.6383e-01,\n",
      "           -9.5146e-01, -7.1126e-01],\n",
      "          ...,\n",
      "          [-7.2533e-01, -6.1258e-01, -8.3035e-01,  ..., -7.6106e-01,\n",
      "           -8.5967e-01, -6.6118e-01],\n",
      "          [-6.5762e-01, -5.1108e-01, -4.9050e-01,  ..., -8.4978e-01,\n",
      "           -7.2590e-01, -2.8173e-01],\n",
      "          [-4.2541e-01, -5.4713e-01, -6.3999e-01,  ..., -5.4531e-01,\n",
      "           -5.3170e-01, -2.6037e-01]],\n",
      "\n",
      "         [[ 3.9579e-01,  6.4440e-01,  8.6136e-01,  ..., -9.2853e-01,\n",
      "           -9.4814e-01, -6.1509e-01],\n",
      "          [-1.0063e-02,  7.6085e-01,  1.2069e-01,  ..., -9.8738e-01,\n",
      "           -9.6984e-01, -8.7699e-01],\n",
      "          [ 9.0024e-01,  7.6984e-01, -6.7489e-01,  ..., -9.8182e-01,\n",
      "           -9.6442e-01, -8.3867e-01],\n",
      "          ...,\n",
      "          [-6.5365e-01, -8.7620e-01, -9.0317e-01,  ..., -9.4876e-01,\n",
      "           -9.5318e-01, -7.4797e-01],\n",
      "          [-8.6102e-01, -6.9120e-01, -8.6798e-01,  ..., -7.2345e-01,\n",
      "           -7.6365e-01, -5.8413e-01],\n",
      "          [-5.4341e-01, -7.1640e-01, -8.1875e-01,  ..., -7.6938e-01,\n",
      "           -6.7114e-01, -4.2579e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 9.9978e-01,  9.9946e-01,  1.0000e+00,  ...,  3.5900e-01,\n",
      "            2.1693e-01, -1.6320e-01],\n",
      "          [ 9.9995e-01,  9.9976e-01,  9.9997e-01,  ...,  8.9778e-01,\n",
      "            7.8456e-01,  4.7436e-01],\n",
      "          [ 1.0000e+00,  9.9999e-01,  1.0000e+00,  ...,  9.7473e-01,\n",
      "            9.8044e-01,  2.0006e-01],\n",
      "          ...,\n",
      "          [-8.4575e-01, -8.9668e-01, -9.5953e-01,  ..., -7.7384e-01,\n",
      "           -7.9352e-01, -5.6879e-01],\n",
      "          [-7.2863e-01, -9.5551e-01, -8.1859e-01,  ..., -8.7713e-01,\n",
      "           -1.3757e-01, -5.8723e-01],\n",
      "          [-7.6418e-01, -5.6256e-01, -9.1682e-01,  ..., -3.7013e-01,\n",
      "           -6.6786e-01, -3.4004e-01]],\n",
      "\n",
      "         [[ 9.9756e-01,  9.9997e-01,  1.0000e+00,  ...,  8.0640e-01,\n",
      "           -1.1142e-01, -5.6959e-02],\n",
      "          [ 9.9988e-01,  9.9998e-01,  1.0000e+00,  ...,  9.2153e-01,\n",
      "            6.6916e-01, -4.7413e-01],\n",
      "          [ 9.9999e-01,  1.0000e+00,  1.0000e+00,  ...,  9.3595e-01,\n",
      "            8.2089e-01,  5.4471e-01],\n",
      "          ...,\n",
      "          [-9.5431e-01, -9.2774e-01, -9.8432e-01,  ..., -6.9833e-01,\n",
      "           -9.2246e-01, -7.0185e-01],\n",
      "          [-8.0884e-01, -8.1926e-01, -8.9664e-01,  ..., -8.8376e-01,\n",
      "           -7.5693e-01, -2.7713e-01],\n",
      "          [-5.9453e-01, -7.0880e-01, -8.2776e-01,  ..., -5.2147e-01,\n",
      "           -5.4712e-01, -2.7773e-01]],\n",
      "\n",
      "         [[ 9.6903e-01,  9.8027e-01,  9.9991e-01,  ...,  8.2698e-01,\n",
      "            8.9510e-01,  4.7369e-01],\n",
      "          [ 8.1412e-01,  9.9988e-01,  9.7629e-01,  ...,  9.1266e-01,\n",
      "            5.2686e-01,  2.0617e-01],\n",
      "          [ 9.9969e-01,  9.9933e-01,  9.1850e-01,  ...,  8.5959e-01,\n",
      "            9.8827e-01,  4.6855e-01],\n",
      "          ...,\n",
      "          [-9.0269e-01, -9.7314e-01, -9.9382e-01,  ..., -9.5518e-01,\n",
      "           -9.6313e-01, -7.5297e-01],\n",
      "          [-9.7154e-01, -9.1990e-01, -9.8080e-01,  ..., -7.0128e-01,\n",
      "           -7.4295e-01, -6.1120e-01],\n",
      "          [-6.9784e-01, -8.6822e-01, -9.4557e-01,  ..., -7.9005e-01,\n",
      "           -6.8121e-01, -4.3154e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5403e-01,  1.7475e-01,  4.1731e-01,  ..., -9.7730e-01,\n",
      "           -6.2391e-01, -8.2035e-01],\n",
      "          [ 2.9911e-01,  2.9450e-01,  5.1712e-01,  ..., -9.7946e-01,\n",
      "           -9.2890e-01, -7.7785e-01],\n",
      "          [ 2.8432e-01,  2.3151e-01,  5.0301e-01,  ..., -9.8681e-01,\n",
      "           -9.0406e-01, -9.0212e-01],\n",
      "          ...,\n",
      "          [-6.1417e-01, -7.5248e-01, -8.5752e-01,  ..., -8.3020e-01,\n",
      "           -7.5964e-01, -6.1381e-01],\n",
      "          [-6.6721e-01, -8.7674e-01, -5.4405e-01,  ..., -9.0164e-01,\n",
      "           -2.2763e-01, -6.3065e-01],\n",
      "          [-6.4751e-01, -3.9905e-01, -8.3634e-01,  ..., -4.0282e-01,\n",
      "           -5.9438e-01, -3.3284e-01]],\n",
      "\n",
      "         [[-1.5547e-01,  1.1466e-01,  3.3996e-01,  ..., -9.6999e-01,\n",
      "           -9.0797e-01, -6.2135e-01],\n",
      "          [-2.5019e-02,  1.4899e-01, -1.8434e-02,  ..., -9.1587e-01,\n",
      "           -9.2014e-01, -7.8898e-01],\n",
      "          [ 2.3448e-01,  2.7858e-01,  5.5181e-01,  ..., -9.6149e-01,\n",
      "           -9.5064e-01, -7.3323e-01],\n",
      "          ...,\n",
      "          [-8.7615e-01, -7.4742e-01, -9.2983e-01,  ..., -8.1434e-01,\n",
      "           -8.0491e-01, -6.3490e-01],\n",
      "          [-7.4086e-01, -6.5747e-01, -6.3926e-01,  ..., -7.9378e-01,\n",
      "           -6.9430e-01, -3.5050e-01],\n",
      "          [-4.8570e-01, -6.1685e-01, -7.1535e-01,  ..., -5.7919e-01,\n",
      "           -5.5859e-01, -2.8828e-01]],\n",
      "\n",
      "         [[-1.4700e-01,  6.8387e-02, -1.7692e-02,  ..., -9.0929e-01,\n",
      "           -9.1311e-01, -6.0699e-01],\n",
      "          [-1.2444e-01, -2.2327e-01, -2.3626e-01,  ..., -9.8177e-01,\n",
      "           -9.6572e-01, -8.6902e-01],\n",
      "          [-3.0384e-01, -5.1196e-03, -2.5423e-01,  ..., -9.8165e-01,\n",
      "           -9.5620e-01, -8.3444e-01],\n",
      "          ...,\n",
      "          [-7.6513e-01, -9.2265e-01, -9.5122e-01,  ..., -9.5093e-01,\n",
      "           -9.3570e-01, -7.7299e-01],\n",
      "          [-9.2314e-01, -7.7708e-01, -9.3128e-01,  ..., -7.8356e-01,\n",
      "           -7.9567e-01, -5.9471e-01],\n",
      "          [-5.9619e-01, -7.8179e-01, -8.7664e-01,  ..., -7.8535e-01,\n",
      "           -6.9309e-01, -4.6704e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.3932e-01, -9.1528e-01, -7.9108e-01,  ..., -4.4286e-01,\n",
      "            9.5644e-03, -5.7655e-01],\n",
      "          [-8.4515e-01, -8.7973e-01, -9.3497e-01,  ...,  9.3825e-03,\n",
      "            1.1639e-01, -3.1295e-01],\n",
      "          [-7.9872e-01, -9.2298e-01, -8.2529e-01,  ...,  1.8512e-01,\n",
      "            3.8523e-01, -6.1806e-01],\n",
      "          ...,\n",
      "          [-8.7723e-01, -9.2640e-01, -9.8772e-01,  ..., -8.0983e-01,\n",
      "           -7.3752e-01, -5.9574e-01],\n",
      "          [-8.7927e-01, -9.7853e-01, -9.1319e-01,  ..., -8.8972e-01,\n",
      "           -1.9387e-01, -6.1327e-01],\n",
      "          [-8.0578e-01, -6.3907e-01, -9.4556e-01,  ..., -3.8415e-01,\n",
      "           -5.7623e-01, -3.2321e-01]],\n",
      "\n",
      "         [[-7.6658e-01, -9.3887e-01, -9.0417e-01,  ..., -1.0755e-01,\n",
      "           -4.2435e-01, -4.0249e-01],\n",
      "          [-9.1294e-01, -8.7672e-01, -9.6309e-01,  ...,  2.0551e-01,\n",
      "           -3.6230e-01, -6.2039e-01],\n",
      "          [-7.7302e-01, -7.7610e-01, -9.3294e-01,  ...,  1.7428e-01,\n",
      "           -2.4884e-01, -3.0767e-01],\n",
      "          ...,\n",
      "          [-9.8222e-01, -9.4276e-01, -9.9602e-01,  ..., -7.9096e-01,\n",
      "           -7.8277e-01, -6.1647e-01],\n",
      "          [-8.2086e-01, -8.6511e-01, -8.9989e-01,  ..., -7.7351e-01,\n",
      "           -6.6714e-01, -3.3302e-01],\n",
      "          [-6.2736e-01, -7.3664e-01, -8.4482e-01,  ..., -5.6719e-01,\n",
      "           -5.4376e-01, -2.7661e-01]],\n",
      "\n",
      "         [[-8.6796e-01, -8.3530e-01, -9.5545e-01,  ...,  2.4630e-01,\n",
      "            2.8025e-01, -3.0000e-01],\n",
      "          [-8.7004e-01, -9.6689e-01, -9.8614e-01,  ..., -8.6273e-02,\n",
      "           -5.9395e-01, -5.8844e-01],\n",
      "          [-9.5502e-01, -9.3567e-01, -9.5025e-01,  ..., -3.2472e-01,\n",
      "            3.4304e-01, -4.3915e-01],\n",
      "          ...,\n",
      "          [-9.2424e-01, -9.7348e-01, -9.9629e-01,  ..., -9.4359e-01,\n",
      "           -9.2674e-01, -7.5643e-01],\n",
      "          [-9.7656e-01, -9.2786e-01, -9.8496e-01,  ..., -7.6506e-01,\n",
      "           -7.7998e-01, -5.7657e-01],\n",
      "          [-6.7962e-01, -8.9091e-01, -9.5055e-01,  ..., -7.7685e-01,\n",
      "           -6.7988e-01, -4.5657e-01]]]])\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "batch: tensor([[[-0.8621, -0.9642, -0.8919,  ..., -0.9515, -0.5590, -0.7583],\n",
      "         [-0.9235, -0.9462, -0.9715,  ..., -0.9620, -0.8940, -0.7391],\n",
      "         [-0.9166, -0.9731, -0.9482,  ..., -0.9655, -0.8217, -0.8047],\n",
      "         ...,\n",
      "         [-0.8538, -0.9082, -0.9748,  ..., -0.8456, -0.7528, -0.6110],\n",
      "         [-0.7818, -0.9606, -0.8659,  ..., -0.9186, -0.1990, -0.6228],\n",
      "         [-0.7686, -0.5694, -0.9245,  ..., -0.4266, -0.6015, -0.3319]],\n",
      "\n",
      "        [[-0.8755, -0.9706, -0.9534,  ..., -0.9524, -0.8451, -0.5289],\n",
      "         [-0.9670, -0.9375, -0.9875,  ..., -0.8855, -0.8886, -0.7349],\n",
      "         [-0.9002, -0.9203, -0.9883,  ..., -0.9132, -0.8826, -0.6235],\n",
      "         ...,\n",
      "         [-0.9683, -0.9324, -0.9908,  ..., -0.8531, -0.8134, -0.6637],\n",
      "         [-0.8426, -0.8416, -0.8914,  ..., -0.8742, -0.8057, -0.3439],\n",
      "         [-0.5878, -0.7082, -0.8351,  ..., -0.5831, -0.5631, -0.2851]],\n",
      "\n",
      "        [[-0.9256, -0.8965, -0.9768,  ..., -0.8835, -0.8858, -0.5502],\n",
      "         [-0.9316, -0.9851, -0.9952,  ..., -0.9714, -0.9490, -0.8443],\n",
      "         [-0.9832, -0.9775, -0.9895,  ..., -0.9632, -0.9137, -0.7883],\n",
      "         ...,\n",
      "         [-0.9014, -0.9753, -0.9950,  ..., -0.9644, -0.9523, -0.7884],\n",
      "         [-0.9745, -0.9199, -0.9847,  ..., -0.8323, -0.8407, -0.6084],\n",
      "         [-0.6931, -0.8681, -0.9504,  ..., -0.8023, -0.6995, -0.4643]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/topaz.aharon/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/functional.py:3118: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Got 3D input, but bilinear mode needs 4D input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-542948ce2628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvanilla_gan\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mv_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_gwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v_gan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_gan_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##inception score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# mean , scores = inception_score(v_gen, cuda=True, batch_size=32, resize=False, splits=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/hw4/project/train_gan_model.py\u001b[0m in \u001b[0;36mtrain_gan_model\u001b[0;34m(device, ds_gwb, modelCodeModule, checkpoint_file_suffix, hyperparams)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# inception_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The size of samples:{samples}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inception score of {checkpoint_file_suffix}: scores are: {scores} and mean score is: {mean}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/hw4/project/score_inception.py\u001b[0m in \u001b[0;36minception_score\u001b[0;34m(dataloader, cuda, batch_size, resize, splits)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mbatch_size_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Now compute the mean kl-div\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/hw4/project/score_inception.py\u001b[0m in \u001b[0;36mget_pred\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_linear1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 3D input, but bilinear mode needs 4D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3157\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 3D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Got 3D input, but bilinear mode needs 4D input"
     ]
    }
   ],
   "source": [
    "import project.vanilla_gan as v_gan\n",
    "v_gen = train_gan_model(device, ds_gwb, v_gan, 'v_gan', v_gan_hyperparams())\n",
    "\n",
    "##inception score\n",
    "# mean , scores = inception_score(v_gen, cuda=True, batch_size=32, resize=False, splits=1)\n",
    "# print(f'$$$$$$$$$$$$$$$ scores are {scores} and mean score is{mean}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project.spectral_norm_gan as sn_gan\n",
    "sn_gen = train_gan_model(device, ds_gwb, sn_gan,'sn_gan',sn_gan_hyperparams())\n",
    "print(sn_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project.wass_gan as w_gan\n",
    "w_gen = train_gan_model(device, ds_gwb, w_gan,'w_gan',w_gan_hyperparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project.spectral_norm_wass_gan as sn_w_gan\n",
    "sn_plus_w_gen = train_gan_model(device, ds_gwb, sn_w_gan,'sn_w_gan',w_gan_hyperparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
