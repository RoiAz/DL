r"""
Use this module to write your answers to the questions in the notebook.

Note: Inside the answer strings you can use Markdown format and also LaTeX
math (delimited with $$).
"""

# ==============
# Part 1 answers

part1_q1 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part1_q2 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

# ==============
# Part 2 answers

part2_q1 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part2_q2 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

# ==============

# ==============
# Part 3 answers

part3_q1 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part3_q2 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part3_q3 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

# ==============

# ==============
# Part 4 answers

part4_q1 = r"""
**Your answer:**
The ideal pattern we want to see is test-set predictions with error 0 - meaining located on y-y.pred=0 line.
This scenario is not likley possible, so we will want similar result on the train-set but not exactly the same to avoid overfit.
We can see the plot above have pretty god generalization be its possible to do better.
From the comparing we can see that we got better results after the K-fold process - more instances are close to error 0.

"""

part4_q2 = r"""
**Your answer:**
1. Yes - The model is stil lineare because the parmaters still linear.
    for examlpe, we can mark the feature $x1^{2} = x'$ and as you can see its still linear
    
2. Theoretically we can, but a great classifier might be a complex relationship between the features.

3. With non-linear features we create another dimensions where the linear classifier isn't enough for their representation, the separation is still linear thatfore its still hyperplane

"""

part4_q3 = r"""
**Your answer:**
1. In log space the ùúÜ values are samller from linear space, if we go linear ùúÜ will get bigg values and the weights will have small values as result. In the extreme case the weights will be 0 and the model will have underfitting (higher ùúÜ less complex model).

2. len(degree_range) * len(lambda_range) * k_fold = 3 * 20 * 3 = 180

"""

# ==============
